{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OMNIFALL_ROOT = \"/lsdf/data/activity/fall_detection/cvhci_fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.data.video_dataset import OmnifallVideoDataset\n",
    "\n",
    "dataset_config = {\n",
    "    \"video_root\": f\"{OMNIFALL_ROOT}/OOPS/video\",\n",
    "    \"annotations_file\": \"hf://simplexsigil2/omnifall/labels/OOPS.csv\",\n",
    "    \"split_root\": \"hf://simplexsigil2/omnifall/splits\",\n",
    "    \"dataset_name\": \"OOPS\",\n",
    "    \"mode\": \"test\",  # Start with test set (smaller)\n",
    "    \"split\": \"cs\",  # Cross-subject split\n",
    "    \"target_fps\": 8.0,  # Low FPS for quick testing\n",
    "    \"vid_frame_count\": 8,\n",
    "    \"data_fps\": 30.0,  # OOPS videos are 30 FPS\n",
    "    \"ext\": \".mp4\",\n",
    "    \"fast\": True,\n",
    "    \"size\": (320, 567),  # Resize frames to 320x567 for faster processing\n",
    "}\n",
    "\n",
    "print(\"\\nDataset Configuration:\")\n",
    "for key, value in dataset_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nCreating OmnifallVideoDataset...\")\n",
    "try:\n",
    "    dataset = OmnifallVideoDataset(**dataset_config)\n",
    "    print(\"✓ Dataset created successfully!\")\n",
    "    print(f\"\\n{dataset}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create dataset: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "sample[\"video\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get video tensor from sample - shape is (C, T, H, W)\n",
    "video = sample[\"video\"]\n",
    "\n",
    "# Convert from (C, T, H, W) to (T, H, W, C) for visualization\n",
    "frames = video.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "# Normalize to 0-255 range if needed\n",
    "frames = frames.astype(\"uint8\")\n",
    "\n",
    "# Plot a grid of frames\n",
    "num_frames = frames.shape[0]\n",
    "cols = 4\n",
    "rows = (num_frames + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    if i < num_frames:\n",
    "        axes[i].imshow(frames[i])\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Label: {sample['label_str']} | Video: {sample['video_path']}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "model_checkpoint = \"Qwen/Qwen3-VL-2B-Thinking\"\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    do_sample_frames=False,\n",
    ")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_checkpoint, dtype=\"bfloat16\", device_map=\"auto\", attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280 (32× spatial compression)\n",
    "# processor.image_processor.size = {\"longest_edge\": 1280*32*32, \"shortest_edge\": 256*32*32}\n",
    "\n",
    "# budget for video processor, we can set the number of visual tokens of a single video to 256-16384 (32× spatial compression + 2× temporal compression)\n",
    "# processor.video_processor.size = {\"longest_edge\": 16384*32*32*2, \"shortest_edge\": 256*32*32*2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.video_utils import VideoMetadata\n",
    "\n",
    "metadata = VideoMetadata(\n",
    "    total_num_frames=len(frames),\n",
    "    fps=8.0,\n",
    "    frames_indices=list(range(len(frames))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": frames,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Classify the action of the main subject in the video.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    video_metadata=metadata,\n",
    ").to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values_videos\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=10000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
